{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "from smac import HyperparameterOptimizationFacade, Scenario\n",
    "from ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float, Integer\n",
    "from ConfigSpace import (\n",
    "    ConfigurationSpace, UniformIntegerHyperparameter, UniformFloatHyperparameter, CategoricalHyperparameter\n",
    ")\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "model_outputs = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Different Bayesian Optimization Libraries and Grid Search\n",
    "\n",
    "In this notebook, we will explore different libraries and methods for optimizing hyperparameters of machine learning models. We will compare grid search, Bayesian optimization with Gaussian Processes (GPs) using the `BayesianOptimization` library, Bayesian optimization with tree-structured Parzen estimators using the `Hyperopt` library, and Bayesian optimization with Random Forest regressions using the `SMAC3` library. These methods will be applied to optimize the hyperparameters of a Random Forest classifier from `sklearn` using the Iris dataset. This builds upon our previous understanding of Gaussian Processes and Bayesian optimization from the earlier notebooks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Randomized Search with Cross-Validation\n",
    "\n",
    "Grid search is a widely used technique for hyperparameter optimization. It involves an exhaustive search through a manually specified subset of the hyperparameter space. Although grid search can be computationally expensive and time-consuming, it is a simple and straightforward method for finding the best hyperparameters for a given model.\n",
    "\n",
    "We will implement a grid search using the `GridSearchCV` function from `sklearn.model_selection`. This function allows us to perform an exhaustive search over a specified parameter grid, using cross-validation to assess the performance of each combination of hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:\n",
      "n_estimators: 49\n",
      "max_depth: 2\n",
      "min_samples_split: 0.1637839854978797\n",
      "min_samples_leaf: 0.19800723366913853\n",
      "loss function: -0.9466666666666665\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(10, 150, dtype=int),\n",
    "    'max_depth': np.arange(1, 20, dtype=int),\n",
    "    'min_samples_split': np.random.uniform(0.1, 1, size=100),\n",
    "    'min_samples_leaf': np.random.uniform(0.1, 0.5, size=100),\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Run the randomized grid search\n",
    "random_search = RandomizedSearchCV(\n",
    "    clf, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1\n",
    ")\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(\"n_estimators:\", random_search.best_params_['n_estimators'])\n",
    "print(\"max_depth:\", random_search.best_params_['max_depth'])\n",
    "print(\"min_samples_split:\", random_search.best_params_['min_samples_split'])\n",
    "print(\"min_samples_leaf:\", random_search.best_params_['min_samples_leaf'])\n",
    "\n",
    "clf = RandomForestClassifier(**random_search.best_params_)\n",
    "acc = cross_val_score(clf, X, y, cv=5).mean()\n",
    "print(\"loss function:\", -acc)\n",
    "\n",
    "model_outputs['RandomizedSearchCV'] = {\n",
    "    \"n_estimators\": random_search.best_params_['n_estimators'],\n",
    "    \"max_depth\": random_search.best_params_['max_depth'],\n",
    "    \"min_samples_split\": random_search.best_params_['min_samples_split'],\n",
    "    \"min_samples_leaf\": random_search.best_params_['min_samples_leaf'],\n",
    "    \"loss\": -acc\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimisation with Gaussian Processes (Github: [BayesianOptimization](https://github.com/fmfn/BayesianOptimization))\n",
    "\n",
    "The `BayesianOptimization` library is a Python package that provides a simple and efficient implementation of Bayesian optimization using Gaussian Processes. It allows us to optimize complex, expensive-to-evaluate functions with minimal function evaluations. The library is particularly useful for optimizing hyperparameters of machine learning models, as it can efficiently explore the hyperparameter space to find the best combination of hyperparameters.\n",
    "\n",
    "We will use the `BayesianOptimization` library to optimize the hyperparameters of the Random Forest classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8.923418089348907, 'min_samples_leaf': 0.38812979737686326, 'min_samples_split': 0.1001029373356104, 'n_estimators': 52.32656016845757} - Target value =  0.3333333333333333\n",
      "{'max_depth': 1.1466772636983, 'min_samples_leaf': 0.1647443736850785, 'min_samples_split': 0.7384321307529985, 'n_estimators': 117.06316025882413} - Target value =  0.3333333333333333\n",
      "{'max_depth': 19.635105236299918, 'min_samples_leaf': 0.35628072635514696, 'min_samples_split': 0.8208045483256007, 'n_estimators': 10.103655336075324} - Target value =  0.3333333333333333\n",
      "{'max_depth': 19.78814732966481, 'min_samples_leaf': 0.4527634743597063, 'min_samples_split': 0.12071819207446398, 'n_estimators': 149.96333229697035} - Target value =  0.3333333333333333\n",
      "{'max_depth': 1.9098897597413482, 'min_samples_leaf': 0.40167210431205813, 'min_samples_split': 0.19577288031842527, 'n_estimators': 10.286754688103606} - Target value =  0.3333333333333333\n",
      "{'max_depth': 1.0162404445650304, 'min_samples_leaf': 0.14738297543500112, 'min_samples_split': 0.5027170148113955, 'n_estimators': 149.72258689347075} - Target value =  0.96\n",
      "{'max_depth': 1.5464916247858485, 'min_samples_leaf': 0.36538537033478347, 'min_samples_split': 0.4839866232711423, 'n_estimators': 148.92944137106986} - Target value =  0.3333333333333333\n",
      "{'max_depth': 19.643394585592738, 'min_samples_leaf': 0.20816693139242892, 'min_samples_split': 0.8279065162627913, 'n_estimators': 72.518306498036} - Target value =  0.3333333333333333\n",
      "{'max_depth': 7.172639466581227, 'min_samples_leaf': 0.35431256647431764, 'min_samples_split': 0.46045459842482606, 'n_estimators': 115.21767198804463} - Target value =  0.3333333333333333\n",
      "{'max_depth': 6.812313192738442, 'min_samples_leaf': 0.3706613216052107, 'min_samples_split': 0.5962435554474824, 'n_estimators': 54.22007349959964} - Target value =  0.3333333333333333\n",
      "{'max_depth': 8.546506085591767, 'min_samples_leaf': 0.17767115856767718, 'min_samples_split': 0.14757037926889838, 'n_estimators': 58.293284365611605} - Target value =  0.9466666666666665\n",
      "{'max_depth': 8.583963608052116, 'min_samples_leaf': 0.2814613944366938, 'min_samples_split': 0.36054686535381564, 'n_estimators': 58.13292268722821} - Target value =  0.7\n",
      "{'max_depth': 7.314700639997452, 'min_samples_leaf': 0.2939170026054512, 'min_samples_split': 0.4751436778194963, 'n_estimators': 124.91623908214359} - Target value =  0.6866666666666668\n",
      "{'max_depth': 5.447084409820311, 'min_samples_leaf': 0.278800742271491, 'min_samples_split': 0.595098741740875, 'n_estimators': 142.8855762396415} - Target value =  0.6933333333333334\n",
      "{'max_depth': 3.239550200478371, 'min_samples_leaf': 0.32021243831067137, 'min_samples_split': 0.5423186264133282, 'n_estimators': 30.43447851603353} - Target value =  0.6599999999999999\n",
      "{'max_depth': 8.287005469602828, 'min_samples_leaf': 0.40052128982896174, 'min_samples_split': 0.1576022702149628, 'n_estimators': 58.19515638151968} - Target value =  0.3333333333333333\n",
      "{'max_depth': 1.1082275358201943, 'min_samples_leaf': 0.1418379067448371, 'min_samples_split': 0.4837982017206761, 'n_estimators': 149.8612282259244} - Target value =  0.9466666666666667\n",
      "{'max_depth': 4.434981873876592, 'min_samples_leaf': 0.2087085177978319, 'min_samples_split': 0.823186555886755, 'n_estimators': 100.72539610307324} - Target value =  0.3333333333333333\n",
      "{'max_depth': 8.838388715838704, 'min_samples_leaf': 0.295230910362529, 'min_samples_split': 0.3445953368725834, 'n_estimators': 58.53216127279977} - Target value =  0.7\n",
      "{'max_depth': 1.3390937444401971, 'min_samples_leaf': 0.10355101152800393, 'min_samples_split': 0.8515794226762681, 'n_estimators': 149.76333364925247} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.54279703122174, 'min_samples_leaf': 0.16541068483078086, 'min_samples_split': 0.11352989574905824, 'n_estimators': 32.53841920316627} - Target value =  0.9466666666666665\n",
      "{'max_depth': 19.73418409931721, 'min_samples_leaf': 0.3833310559593125, 'min_samples_split': 0.6242617637599471, 'n_estimators': 114.45741918847817} - Target value =  0.3333333333333333\n",
      "{'max_depth': 18.080981633299167, 'min_samples_leaf': 0.28088777952299937, 'min_samples_split': 0.45929337822811367, 'n_estimators': 53.613949294172535} - Target value =  0.7066666666666667\n",
      "{'max_depth': 1.035072759946014, 'min_samples_leaf': 0.37758269609084794, 'min_samples_split': 0.5660683347712048, 'n_estimators': 149.45332386415754} - Target value =  0.3333333333333333\n",
      "{'max_depth': 8.679278776710923, 'min_samples_leaf': 0.1764152057567598, 'min_samples_split': 0.12056709977403908, 'n_estimators': 58.1197702478566} - Target value =  0.9533333333333334\n",
      "{'max_depth': 8.511536480476543, 'min_samples_leaf': 0.1, 'min_samples_split': 0.1, 'n_estimators': 58.113621688640755} - Target value =  0.9466666666666665\n",
      "{'max_depth': 16.275608148930036, 'min_samples_leaf': 0.14964396767588256, 'min_samples_split': 0.27030598385053933, 'n_estimators': 32.75694880609768} - Target value =  0.96\n",
      "{'max_depth': 11.246026004809622, 'min_samples_leaf': 0.25682909568932755, 'min_samples_split': 0.9435231043665979, 'n_estimators': 69.03134982581824} - Target value =  0.3333333333333333\n",
      "{'max_depth': 10.281481548181954, 'min_samples_leaf': 0.12567793628928325, 'min_samples_split': 0.3966730604924875, 'n_estimators': 99.62531536906728} - Target value =  0.9466666666666665\n",
      "{'max_depth': 10.175238484733672, 'min_samples_leaf': 0.41704760607570557, 'min_samples_split': 0.16444460667550576, 'n_estimators': 99.46500446286122} - Target value =  0.3333333333333333\n",
      "{'max_depth': 13.446203048770984, 'min_samples_leaf': 0.18710027925794148, 'min_samples_split': 0.7323896377127147, 'n_estimators': 136.97967531815937} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.311628513299127, 'min_samples_leaf': 0.13553163116149133, 'min_samples_split': 0.3553812024229881, 'n_estimators': 32.3900941395117} - Target value =  0.9466666666666665\n",
      "{'max_depth': 16.60529874607105, 'min_samples_leaf': 0.29970191637653204, 'min_samples_split': 0.5545597065948372, 'n_estimators': 32.882693869427705} - Target value =  0.6866666666666666\n",
      "{'max_depth': 7.833624848420068, 'min_samples_leaf': 0.302777161085908, 'min_samples_split': 0.7523526373401702, 'n_estimators': 82.9302976909714} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.700807273779887, 'min_samples_leaf': 0.29686271399339065, 'min_samples_split': 0.6295895142786234, 'n_estimators': 32.892880911746396} - Target value =  0.6866666666666666\n",
      "{'max_depth': 18.68334143938868, 'min_samples_leaf': 0.46499616437663716, 'min_samples_split': 0.7794484613257918, 'n_estimators': 106.50883279374956} - Target value =  0.3333333333333333\n",
      "{'max_depth': 8.473674964871377, 'min_samples_leaf': 0.47778606656969624, 'min_samples_split': 0.7884454565500748, 'n_estimators': 120.07277135333487} - Target value =  0.3333333333333333\n",
      "{'max_depth': 7.65289609215238, 'min_samples_leaf': 0.47716740366405785, 'min_samples_split': 0.4414125373472092, 'n_estimators': 50.514498405558484} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.50976113570985, 'min_samples_leaf': 0.1934491258913766, 'min_samples_split': 0.4570384889890541, 'n_estimators': 32.593933185364826} - Target value =  0.9533333333333334\n",
      "{'max_depth': 16.689710373395258, 'min_samples_leaf': 0.1, 'min_samples_split': 0.36178001134253623, 'n_estimators': 32.28913653293935} - Target value =  0.9466666666666667\n",
      "{'max_depth': 9.006826593486412, 'min_samples_leaf': 0.26722207799208453, 'min_samples_split': 0.8424393089402801, 'n_estimators': 60.113790425146256} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.513826134191252, 'min_samples_leaf': 0.46938318241577626, 'min_samples_split': 0.2954673905316789, 'n_estimators': 32.35784025897774} - Target value =  0.3333333333333333\n",
      "{'max_depth': 19.8160190042703, 'min_samples_leaf': 0.35761590399765375, 'min_samples_split': 0.17902916730084534, 'n_estimators': 19.886632649347753} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.45259753714361, 'min_samples_leaf': 0.1897977120510099, 'min_samples_split': 0.3613890339903919, 'n_estimators': 32.233408204634124} - Target value =  0.9466666666666667\n",
      "{'max_depth': 16.619735660256374, 'min_samples_leaf': 0.1296124613069022, 'min_samples_split': 0.11798663891671227, 'n_estimators': 32.73944788950755} - Target value =  0.9466666666666665\n",
      "{'max_depth': 8.223094085215344, 'min_samples_leaf': 0.43935833293333515, 'min_samples_split': 0.4698441746545208, 'n_estimators': 62.197316260757454} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.60607681846671, 'min_samples_leaf': 0.3317789756024872, 'min_samples_split': 0.41521132157597407, 'n_estimators': 33.073665360899966} - Target value =  0.6599999999999999\n",
      "{'max_depth': 16.12740648196141, 'min_samples_leaf': 0.19745754703760177, 'min_samples_split': 0.38570130498998556, 'n_estimators': 32.09813300505365} - Target value =  0.96\n",
      "{'max_depth': 10.31258846512913, 'min_samples_leaf': 0.12640707767406256, 'min_samples_split': 0.39609179355510704, 'n_estimators': 99.94984497598064} - Target value =  0.9466666666666665\n",
      "{'max_depth': 6.444741576334142, 'min_samples_leaf': 0.1415468444266771, 'min_samples_split': 0.8682097661511465, 'n_estimators': 29.761356570102734} - Target value =  0.3333333333333333\n",
      "{'max_depth': 10.390476939539532, 'min_samples_leaf': 0.15285785189999485, 'min_samples_split': 0.7773640612010436, 'n_estimators': 99.82171467533433} - Target value =  0.3333333333333333\n",
      "{'max_depth': 18.84459527316742, 'min_samples_leaf': 0.48875663980506623, 'min_samples_split': 0.32336139235978245, 'n_estimators': 29.384085005109604} - Target value =  0.3333333333333333\n",
      "{'max_depth': 15.97101655855266, 'min_samples_leaf': 0.36097500728362986, 'min_samples_split': 0.36674988850007273, 'n_estimators': 31.808735813779514} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.28835091009595, 'min_samples_leaf': 0.27135023774295874, 'min_samples_split': 0.7618190707469306, 'n_estimators': 32.279378997569296} - Target value =  0.3333333333333333\n",
      "{'max_depth': 10.58579751056943, 'min_samples_leaf': 0.1698254983135633, 'min_samples_split': 0.33961943622967095, 'n_estimators': 100.00646786746701} - Target value =  0.9466666666666665\n",
      "{'max_depth': 4.021360287053524, 'min_samples_leaf': 0.29137458027564256, 'min_samples_split': 0.5560222670338184, 'n_estimators': 26.959375556123643} - Target value =  0.6933333333333334\n",
      "{'max_depth': 16.860006267987202, 'min_samples_leaf': 0.1, 'min_samples_split': 0.5400383961339682, 'n_estimators': 32.59156551742586} - Target value =  0.8533333333333333\n",
      "{'max_depth': 10.888416550777064, 'min_samples_leaf': 0.23240816595964256, 'min_samples_split': 0.1227491658274753, 'n_estimators': 99.80311768822136} - Target value =  0.8933333333333333\n",
      "{'max_depth': 11.22680272604883, 'min_samples_leaf': 0.21033314857186444, 'min_samples_split': 0.18545787643250788, 'n_estimators': 100.11161180889864} - Target value =  0.9133333333333333\n",
      "{'max_depth': 11.168601478209615, 'min_samples_leaf': 0.1978342721521228, 'min_samples_split': 0.4234462119392265, 'n_estimators': 99.9942107224339} - Target value =  0.96\n",
      "{'max_depth': 11.36980692794439, 'min_samples_leaf': 0.331587619006452, 'min_samples_split': 0.7500661898836971, 'n_estimators': 100.17704110981197} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.186991182471438, 'min_samples_leaf': 0.20880898597097006, 'min_samples_split': 0.7287753519367266, 'n_estimators': 72.14645690168766} - Target value =  0.3333333333333333\n",
      "{'max_depth': 15.88820414810752, 'min_samples_leaf': 0.2563051255329643, 'min_samples_split': 0.18503078139608592, 'n_estimators': 33.04871975906089} - Target value =  0.8200000000000001\n",
      "{'max_depth': 1.1009993528235706, 'min_samples_leaf': 0.15973193425690507, 'min_samples_split': 0.20852688794393354, 'n_estimators': 149.84889584839334} - Target value =  0.9399999999999998\n",
      "{'max_depth': 18.033666902406537, 'min_samples_leaf': 0.18984424933053817, 'min_samples_split': 0.38612404368147313, 'n_estimators': 53.60436028723143} - Target value =  0.9533333333333334\n",
      "{'max_depth': 16.193999353830876, 'min_samples_leaf': 0.2534493285424959, 'min_samples_split': 0.805242481550072, 'n_estimators': 109.97835730523393} - Target value =  0.3333333333333333\n",
      "{'max_depth': 15.848017791872369, 'min_samples_leaf': 0.300176403272114, 'min_samples_split': 0.138940799686053, 'n_estimators': 32.5312666205474} - Target value =  0.6933333333333334\n",
      "{'max_depth': 17.8754677495513, 'min_samples_leaf': 0.1, 'min_samples_split': 0.14146700177059984, 'n_estimators': 53.572299411091414} - Target value =  0.9466666666666665\n",
      "{'max_depth': 11.82834724958194, 'min_samples_leaf': 0.11393227853627708, 'min_samples_split': 0.8546637813167896, 'n_estimators': 85.64764895074903} - Target value =  0.3333333333333333\n",
      "{'max_depth': 10.047364557903697, 'min_samples_leaf': 0.48218886443995246, 'min_samples_split': 0.33918584301443444, 'n_estimators': 100.37550683383665} - Target value =  0.3333333333333333\n",
      "{'max_depth': 19.113673643907152, 'min_samples_leaf': 0.40647410939775275, 'min_samples_split': 0.9997468518101317, 'n_estimators': 57.09195927935022} - Target value =  0.3333333333333333\n",
      "{'max_depth': 16.374767389542036, 'min_samples_leaf': 0.202236040130037, 'min_samples_split': 0.26705698794185195, 'n_estimators': 32.84053128318023} - Target value =  0.9399999999999998\n",
      "{'max_depth': 10.85072747092342, 'min_samples_leaf': 0.2473308688308539, 'min_samples_split': 0.16180779286710892, 'n_estimators': 100.16038214626201} - Target value =  0.8533333333333333\n",
      "{'max_depth': 11.018693090176104, 'min_samples_leaf': 0.423228521352902, 'min_samples_split': 0.8109996679330418, 'n_estimators': 100.05075875660293} - Target value =  0.3333333333333333\n",
      "{'max_depth': 10.635709097253102, 'min_samples_leaf': 0.30579498259041926, 'min_samples_split': 0.38910202724455634, 'n_estimators': 99.95599022627331} - Target value =  0.6799999999999999\n",
      "{'max_depth': 17.57403057422912, 'min_samples_leaf': 0.2721769102138988, 'min_samples_split': 0.2424915589648084, 'n_estimators': 53.715479670941605} - Target value =  0.74\n",
      "{'max_depth': 16.088421956221385, 'min_samples_leaf': 0.1, 'min_samples_split': 0.5297403625035774, 'n_estimators': 33.02305079863734} - Target value =  0.9533333333333334\n",
      "{'max_depth': 11.148219936514597, 'min_samples_leaf': 0.3486915549068914, 'min_samples_split': 0.25575270170619563, 'n_estimators': 99.54671097696702} - Target value =  0.4666666666666666\n",
      "{'max_depth': 12.478279665151083, 'min_samples_leaf': 0.37602513658545933, 'min_samples_split': 0.6870021333098952, 'n_estimators': 68.11316265408178} - Target value =  0.3333333333333333\n",
      "{'max_depth': 15.831232534258646, 'min_samples_leaf': 0.3893034588527253, 'min_samples_split': 0.5321443508778474, 'n_estimators': 33.04677729389396} - Target value =  0.3333333333333333\n",
      "{'max_depth': 6.237620722220124, 'min_samples_leaf': 0.25717423967659925, 'min_samples_split': 0.6113535401112558, 'n_estimators': 26.749041949189543} - Target value =  0.8466666666666667\n",
      "{'max_depth': 16.22842090339278, 'min_samples_leaf': 0.12916899966797157, 'min_samples_split': 0.2165432934716312, 'n_estimators': 33.33653468394949} - Target value =  0.96\n",
      "{'max_depth': 18.157796351937346, 'min_samples_leaf': 0.18407787147714105, 'min_samples_split': 0.1423835744997175, 'n_estimators': 53.965217937945866} - Target value =  0.96\n",
      "{'max_depth': 15.158489512221642, 'min_samples_leaf': 0.14720008337600363, 'min_samples_split': 0.1097096143451567, 'n_estimators': 92.56038003294533} - Target value =  0.9466666666666665\n",
      "{'max_depth': 19.148718855994744, 'min_samples_leaf': 0.3533491002456456, 'min_samples_split': 0.5124061081361971, 'n_estimators': 124.1504683124874} - Target value =  0.3333333333333333\n",
      "{'max_depth': 7.45780610793137, 'min_samples_leaf': 0.15654955380391536, 'min_samples_split': 0.4675874524672533, 'n_estimators': 124.90133872983168} - Target value =  0.9466666666666667\n",
      "{'max_depth': 18.233501974945355, 'min_samples_leaf': 0.1, 'min_samples_split': 0.11650970543579191, 'n_estimators': 53.61736082898617} - Target value =  0.9533333333333334\n",
      "{'max_depth': 11.302602768588944, 'min_samples_leaf': 0.35079193869228775, 'min_samples_split': 0.5482017747459191, 'n_estimators': 35.575573510741194} - Target value =  0.3333333333333333\n",
      "{'max_depth': 7.848540815627713, 'min_samples_leaf': 0.48907991085412217, 'min_samples_split': 0.6002198346006945, 'n_estimators': 124.95906755179593} - Target value =  0.3333333333333333\n",
      "{'max_depth': 14.70049452112442, 'min_samples_leaf': 0.46680117143251165, 'min_samples_split': 0.6775737051624603, 'n_estimators': 41.755997519174144} - Target value =  0.3333333333333333\n",
      "{'max_depth': 8.729703460378193, 'min_samples_leaf': 0.34335019106672277, 'min_samples_split': 0.3810404785891818, 'n_estimators': 58.496829593531594} - Target value =  0.4133333333333333\n",
      "{'max_depth': 14.98478261092543, 'min_samples_leaf': 0.27243463041905724, 'min_samples_split': 0.23823209819009597, 'n_estimators': 92.43180035991038} - Target value =  0.72\n",
      "{'max_depth': 17.983828653826006, 'min_samples_leaf': 0.3284779403751137, 'min_samples_split': 0.22197889158567224, 'n_estimators': 53.22929228429977} - Target value =  0.6866666666666665\n",
      "{'max_depth': 16.01955592220469, 'min_samples_leaf': 0.15885103315605914, 'min_samples_split': 0.3503245140143004, 'n_estimators': 33.613584646948496} - Target value =  0.9533333333333334\n",
      "{'max_depth': 4.699675596965985, 'min_samples_leaf': 0.4499798609195933, 'min_samples_split': 0.10474090475643855, 'n_estimators': 26.803569390770722} - Target value =  0.3333333333333333\n",
      "{'max_depth': 10.457498365534148, 'min_samples_leaf': 0.2013876729154942, 'min_samples_split': 0.6602895813445488, 'n_estimators': 125.60132922018826} - Target value =  0.9333333333333333\n",
      "{'max_depth': 5.537101198242417, 'min_samples_leaf': 0.2224564589596225, 'min_samples_split': 0.5316176388784242, 'n_estimators': 28.017554692346835} - Target value =  0.8733333333333333\n",
      "{'max_depth': 11.368453019835137, 'min_samples_leaf': 0.18948580087725686, 'min_samples_split': 0.20707413523572776, 'n_estimators': 100.12571521837388} - Target value =  0.9533333333333334\n",
      "{'max_depth': 16.432934139381924, 'min_samples_leaf': 0.39806977361056717, 'min_samples_split': 0.185610590098224, 'n_estimators': 33.64963111003772} - Target value =  0.3333333333333333\n",
      "{'max_depth': 9.198710246343792, 'min_samples_leaf': 0.20895206363517765, 'min_samples_split': 0.2633516054011427, 'n_estimators': 58.26937948896655} - Target value =  0.9266666666666667\n",
      "Best hyperparameters found:\n",
      "n_estimators: 149\n",
      "max_depth: 1\n",
      "min_samples_split: 0.5027170148113955\n",
      "min_samples_leaf: 0.14738297543500112\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter search space\n",
    "pbounds = {\n",
    "    'n_estimators': (10, 150),\n",
    "    'max_depth': (1, 20),\n",
    "    'min_samples_split': (0.1, 1),\n",
    "    'min_samples_leaf': (0.1, 0.5),\n",
    "}\n",
    "\n",
    "# Objective function\n",
    "def objective(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
    "    params = {\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'max_depth': int(max_depth),\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "    }\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    acc = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    return acc\n",
    "\n",
    "# Initialize the Bayesian Optimization with the kernel and acquisition function settings\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    "    verbose=2,\n",
    ") \n",
    "\n",
    "# Set the acquisition function (e.g., 'ucb', 'ei', or 'poi')\n",
    "acquisition_function = 'ucb'\n",
    "kappa = 2.576  # Exploration-exploitation parameter for UCB\n",
    "xi = 0.0      # Exploration-exploitation parameter for EI and POI\n",
    "utility = UtilityFunction(kind=acquisition_function, kappa=kappa, xi=xi)\n",
    "\n",
    "# Run the Bayesian Optimization\n",
    "for _ in range(100):\n",
    "    next_point = optimizer.suggest(utility)\n",
    "    target = objective(**next_point)\n",
    "    optimizer.register(params=next_point, target=target)\n",
    "    print(next_point, \"- Target value = \", target)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(\"n_estimators:\", int(optimizer.max['params']['n_estimators']))\n",
    "print(\"max_depth:\", int(optimizer.max['params']['max_depth']))\n",
    "print(\"min_samples_split:\", optimizer.max['params']['min_samples_split'])\n",
    "print(\"min_samples_leaf:\", optimizer.max['params']['min_samples_leaf'])\n",
    "\n",
    "model_outputs['BayesianOptimization'] = {\n",
    "    \"n_estimators\": int(optimizer.max['params']['n_estimators']),\n",
    "    \"max_depth\": int(optimizer.max['params']['max_depth']),\n",
    "    \"min_samples_split\": optimizer.max['params']['min_samples_split'],\n",
    "    \"min_samples_leaf\": optimizer.max['params']['min_samples_leaf'],\n",
    "    \"loss\": -optimizer.max['target']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Bayesian Optimization with Gaussian Processes I (Github: [Trieste](https://github.com/secondmind-labs/trieste))\n",
    "\n",
    "`Trieste` is an advanced Bayesian optimization library built on top of TensorFlow Probability. It offers a highly flexible and modular architecture, enabling users to easily customize various aspects of the optimization process. With its support for more sophisticated algorithms, `Trieste` is well-suited for tackling large-scale and high-dimensional optimization problems. In addition to traditional Gaussian Processes, `Trieste` allows users to employ more novel algorithms, some of which exhibit improved scalability properties, making it an attractive choice for optimizing complex functions.\n",
    "\n",
    "Although we won't be relying on `Trieste` for this notebook, it is worth exploring if you require more advanced features or need to optimize hyperparameters in more complex machine learning models (see [tutorials](https://secondmind-labs.github.io/trieste/1.1.2/tutorials.html)). The library's flexibility and support for TensorFlow Probability make it a powerful tool for Bayesian optimization tasks that demand greater customization and scalability.\n",
    "\n",
    "`Trieste` also provides an \"[ask-tell](https://secondmind-labs.github.io/trieste/1.1.2/notebooks/ask_tell_optimization.html)\" interface, which gives users greater control over the optimization loop. This interface is particularly useful in situations where allowing Trieste to manage the loop autonomously is not possible or desired. By separating the querying of new points (`ask`) from the reporting of function evaluations (`tell`), users can manually intervene in the optimization process, enabling more fine-grained control and customization.\n",
    "\n",
    "The ask-tell interface is beneficial for scenarios where external factors influence the optimization process, such as hardware constraints or when the objective function requires human input. By employing the ask-tell interface, users can incorporate additional logic into the optimization loop, tailoring the algorithm to better suit their specific use case and requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Bayesian Optimization with Gaussian Processes II (Github: [BOtorch](https://github.com/pytorch/botorch))\n",
    "\n",
    "[BOtorch](https://botorch.org/) is a flexible and efficient Bayesian optimization library built on top of PyTorch, GPyTorch, and Ax. It provides a modular and efficient implementation of Bayesian optimization algorithms, enabling researchers and practitioners to apply these methods to various real-world problems.\n",
    "\n",
    "BOtorch is designed with the following goals in mind:\n",
    "\n",
    "- **Modularity**: BOtorch allows users to build custom optimization algorithms by combining different components such as models, acquisition functions, and optimizers. This allows for rapid prototyping and experimentation.\n",
    "\n",
    "- **Efficiency**: BOtorch leverages the power of PyTorch and GPyTorch for efficient tensor computations and automatic differentiation. This enables the library to handle large-scale problems with thousands of hyperparameters.\n",
    "\n",
    "- **Interoperability**: BOtorch is compatible with other popular libraries, such as scikit-learn, and can be easily integrated into existing machine learning pipelines.\n",
    "\n",
    "In this section, we will not explore how to use BOtorch to optimize the hyperparameters of the RandomForest classifier, as installation of torch can be tricky. Raw code below demonstrates nonetheless how to use Mixed GP models for handling mixed continuous and categorical hyperparameters, showcasing the flexibility and power of the BOtorch library for Bayesian optimization tasks.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from botorch.models.transforms import MixedSingleTask\n",
    "from botorch.models.transforms.input import InputCategoryEmbedding\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the search space\n",
    "bounds = [(10, 200), (1, 10), (2, 10), (1, 10)]\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    n_estimators, max_depth, min_samples_split, min_samples_leaf = params\n",
    "    clf = RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                                 max_depth=int(max_depth),\n",
    "                                 min_samples_split=int(min_samples_split),\n",
    "                                 min_samples_leaf=int(min_samples_leaf))\n",
    "    acc = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    return acc\n",
    "\n",
    "# Initialize the samples and observed function values\n",
    "n_init = 10\n",
    "train_x = torch.rand(n_init, 4)\n",
    "train_x[:, 0] *= (bounds[0][1] - bounds[0][0])\n",
    "train_x[:, 0] += bounds[0][0]\n",
    "train_x[:, 1] = torch.randint(bounds[1][0], bounds[1][1] + 1, (n_init,)).float()\n",
    "train_x[:, 2] *= (bounds[2][1] - bounds[2][0])\n",
    "train_x[:, 2] += bounds[2][0]\n",
    "train_x[:, 3] *= (bounds[3][1] - bounds[3][0])\n",
    "train_x[:, 3] += bounds[3][0]\n",
    "\n",
    "train_y = torch.tensor([objective(train_x[i].numpy()) for i in range(n_init)], dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "n_iter = 30\n",
    "for i in range(n_iter):\n",
    "    # Fit the Mixed GP model\n",
    "    model = SingleTaskGP(train_x, train_y)\n",
    "    emb = InputCategoryEmbedding(1, train_x, bounds)\n",
    "    mixed_model = MixedSingleTask(model, emb)\n",
    "    mll = ExactMarginalLogLikelihood(mixed_model.likelihood, mixed_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    # Define the acquisition function\n",
    "    acq_func = ExpectedImprovement(mixed_model, train_y.max())\n",
    "\n",
    "    # Optimize the acquisition function\n",
    "    candidate, _ = optimize_acqf(\n",
    "        acq_func,\n",
    "        bounds=torch.tensor(bounds, dtype=torch.float32),\n",
    "        q=1,\n",
    "        num_restarts=5,\n",
    "        raw_samples=20\n",
    "    )\n",
    "\n",
    "    # Evaluate the objective function\n",
    "    new_y = torch.tensor(objective(candidate.numpy()), dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "    # Update the training data\n",
    "    train_x = torch.cat([train_x, candidate])\n",
    "    train_y = torch.cat([train_y, new_y])\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = train_x[train_y.argmax()]\n",
    "n_estimators, max_depth, min_samples_split, min_samples_leaf = best_params.numpy()\n",
    "\n",
    "# Train and evaluate the optimized Random Forest\n",
    "clf = RandomForestClassifier(n_estimators=int(n_estimators),\n",
    "                             max_depth=int(max_depth),\n",
    "                             min_samples_split=int(min_samples_split),\n",
    "                             min_samples_leaf=int(min_samples_leaf))\n",
    "acc = cross_val_score(clf, X, y, cv=5).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with tree-structured Parzen Estimators (Github: [Hyperopt](https://github.com/hyperopt/hyperopt))\n",
    "\n",
    "`Hyperopt` is another popular library for performing Bayesian optimization. Instead of using Gaussian Processes, `Hyperopt` employs tree-structured Parzen estimators (TPE) to model the objective function. TPE is a sequential model-based optimization approach that uses adaptive Parzen windows to approximate the true function.\n",
    "\n",
    "We will use the `Hyperopt` library to optimize the hyperparameters of the Random Forest classifier (see [tutorial](https://github.com/hyperopt/hyperopt/wiki/FMin) for more details). A good blogpost on TPE is [this one](https://towardsdatascience.com/building-a-tree-structured-parzen-estimator-from-scratch-kind-of-20ed31770478).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?][INFO][tpe.py:864] build_posterior_wrapper took 0.001367 seconds\n",
      "[INFO][tpe.py:900] TPE using 0 trials\n",
      "  1%|          | 1/100 [00:00<01:11,  1.39trial/s, best loss: -0.6866666666666665][INFO][tpe.py:864] build_posterior_wrapper took 0.001947 seconds\n",
      "[INFO][tpe.py:900] TPE using 1/1 trials with best loss -0.686667\n",
      "  2%|▏         | 2/100 [00:01<01:36,  1.02trial/s, best loss: -0.6866666666666665][INFO][tpe.py:864] build_posterior_wrapper took 0.002398 seconds\n",
      "[INFO][tpe.py:900] TPE using 2/2 trials with best loss -0.686667\n",
      "  3%|▎         | 3/100 [00:02<01:34,  1.03trial/s, best loss: -0.6866666666666665][INFO][tpe.py:864] build_posterior_wrapper took 0.001095 seconds\n",
      "[INFO][tpe.py:900] TPE using 3/3 trials with best loss -0.686667\n",
      "  4%|▍         | 4/100 [00:03<01:37,  1.02s/trial, best loss: -0.6866666666666665][INFO][tpe.py:864] build_posterior_wrapper took 0.002478 seconds\n",
      "[INFO][tpe.py:900] TPE using 4/4 trials with best loss -0.686667\n",
      "  5%|▌         | 5/100 [00:04<01:15,  1.26trial/s, best loss: -0.6866666666666665][INFO][tpe.py:864] build_posterior_wrapper took 0.001470 seconds\n",
      "[INFO][tpe.py:900] TPE using 5/5 trials with best loss -0.686667\n",
      "  6%|▌         | 6/100 [00:05<01:17,  1.22trial/s, best loss: -0.6866666666666665][INFO][tpe.py:864] build_posterior_wrapper took 0.001687 seconds\n",
      "[INFO][tpe.py:900] TPE using 6/6 trials with best loss -0.686667\n",
      "  7%|▋         | 7/100 [00:05<01:15,  1.23trial/s, best loss: -0.8200000000000001][INFO][tpe.py:864] build_posterior_wrapper took 0.002019 seconds\n",
      "[INFO][tpe.py:900] TPE using 7/7 trials with best loss -0.820000\n",
      "  8%|▊         | 8/100 [00:06<01:04,  1.42trial/s, best loss: -0.8200000000000001][INFO][tpe.py:864] build_posterior_wrapper took 0.002039 seconds\n",
      "[INFO][tpe.py:900] TPE using 8/8 trials with best loss -0.820000\n",
      "  9%|▉         | 9/100 [00:07<01:07,  1.35trial/s, best loss: -0.8200000000000001][INFO][tpe.py:864] build_posterior_wrapper took 0.001356 seconds\n",
      "[INFO][tpe.py:900] TPE using 9/9 trials with best loss -0.820000\n",
      " 10%|█         | 10/100 [00:08<01:15,  1.19trial/s, best loss: -0.8200000000000001][INFO][tpe.py:864] build_posterior_wrapper took 0.002080 seconds\n",
      "[INFO][tpe.py:900] TPE using 10/10 trials with best loss -0.820000\n",
      " 11%|█         | 11/100 [00:09<01:27,  1.02trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.002464 seconds\n",
      "[INFO][tpe.py:900] TPE using 11/11 trials with best loss -0.946667\n",
      " 12%|█▏        | 12/100 [00:10<01:17,  1.13trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.002361 seconds\n",
      "[INFO][tpe.py:900] TPE using 12/12 trials with best loss -0.946667\n",
      " 13%|█▎        | 13/100 [00:11<01:13,  1.18trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.001183 seconds\n",
      "[INFO][tpe.py:900] TPE using 13/13 trials with best loss -0.946667\n",
      " 14%|█▍        | 14/100 [00:11<01:13,  1.17trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.002202 seconds\n",
      "[INFO][tpe.py:900] TPE using 14/14 trials with best loss -0.946667\n",
      " 15%|█▌        | 15/100 [00:12<01:02,  1.36trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.004358 seconds\n",
      "[INFO][tpe.py:900] TPE using 15/15 trials with best loss -0.946667\n",
      " 16%|█▌        | 16/100 [00:12<00:56,  1.49trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.001499 seconds\n",
      "[INFO][tpe.py:900] TPE using 16/16 trials with best loss -0.946667\n",
      " 17%|█▋        | 17/100 [00:13<00:58,  1.43trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.001228 seconds\n",
      "[INFO][tpe.py:900] TPE using 17/17 trials with best loss -0.946667\n",
      " 18%|█▊        | 18/100 [00:14<01:05,  1.26trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.000897 seconds\n",
      "[INFO][tpe.py:900] TPE using 18/18 trials with best loss -0.946667\n",
      " 19%|█▉        | 19/100 [00:15<01:08,  1.17trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.000844 seconds\n",
      "[INFO][tpe.py:900] TPE using 19/19 trials with best loss -0.946667\n",
      " 20%|██        | 20/100 [00:15<00:53,  1.50trial/s, best loss: -0.9466666666666667][INFO][tpe.py:864] build_posterior_wrapper took 0.000804 seconds\n",
      "[INFO][tpe.py:900] TPE using 20/20 trials with best loss -0.946667\n",
      " 21%|██        | 21/100 [00:16<00:40,  1.97trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.000998 seconds\n",
      "[INFO][tpe.py:900] TPE using 21/21 trials with best loss -0.953333\n",
      " 22%|██▏       | 22/100 [00:17<00:51,  1.52trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.001089 seconds\n",
      "[INFO][tpe.py:900] TPE using 22/22 trials with best loss -0.953333\n",
      " 23%|██▎       | 23/100 [00:17<00:38,  2.01trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.001818 seconds\n",
      "[INFO][tpe.py:900] TPE using 23/23 trials with best loss -0.953333\n",
      " 24%|██▍       | 24/100 [00:17<00:30,  2.48trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.000998 seconds\n",
      "[INFO][tpe.py:900] TPE using 24/24 trials with best loss -0.953333\n",
      " 25%|██▌       | 25/100 [00:17<00:24,  3.08trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.000827 seconds\n",
      "[INFO][tpe.py:900] TPE using 25/25 trials with best loss -0.953333\n",
      " 26%|██▌       | 26/100 [00:18<00:28,  2.63trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.001341 seconds\n",
      "[INFO][tpe.py:900] TPE using 26/26 trials with best loss -0.953333\n",
      " 27%|██▋       | 27/100 [00:18<00:29,  2.52trial/s, best loss: -0.9533333333333334][INFO][tpe.py:864] build_posterior_wrapper took 0.002743 seconds\n",
      "[INFO][tpe.py:900] TPE using 27/27 trials with best loss -0.953333\n",
      " 28%|██▊       | 28/100 [00:18<00:25,  2.78trial/s, best loss: -0.96]              [INFO][tpe.py:864] build_posterior_wrapper took 0.000943 seconds\n",
      "[INFO][tpe.py:900] TPE using 28/28 trials with best loss -0.960000\n",
      " 29%|██▉       | 29/100 [00:19<00:25,  2.78trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.001353 seconds\n",
      "[INFO][tpe.py:900] TPE using 29/29 trials with best loss -0.960000\n",
      " 30%|███       | 30/100 [00:19<00:23,  2.97trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.001857 seconds\n",
      "[INFO][tpe.py:900] TPE using 30/30 trials with best loss -0.960000\n",
      " 31%|███       | 31/100 [00:19<00:21,  3.14trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000759 seconds\n",
      "[INFO][tpe.py:900] TPE using 31/31 trials with best loss -0.960000\n",
      " 32%|███▏      | 32/100 [00:19<00:18,  3.70trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.002480 seconds\n",
      "[INFO][tpe.py:900] TPE using 32/32 trials with best loss -0.960000\n",
      " 33%|███▎      | 33/100 [00:20<00:20,  3.20trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000706 seconds\n",
      "[INFO][tpe.py:900] TPE using 33/33 trials with best loss -0.960000\n",
      " 34%|███▍      | 34/100 [00:20<00:17,  3.74trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000798 seconds\n",
      "[INFO][tpe.py:900] TPE using 34/34 trials with best loss -0.960000\n",
      " 35%|███▌      | 35/100 [00:20<00:17,  3.68trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.001088 seconds\n",
      "[INFO][tpe.py:900] TPE using 35/35 trials with best loss -0.960000\n",
      " 36%|███▌      | 36/100 [00:21<00:19,  3.31trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000721 seconds\n",
      "[INFO][tpe.py:900] TPE using 36/36 trials with best loss -0.960000\n",
      " 37%|███▋      | 37/100 [00:21<00:18,  3.35trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000865 seconds\n",
      "[INFO][tpe.py:900] TPE using 37/37 trials with best loss -0.960000\n",
      " 38%|███▊      | 38/100 [00:21<00:22,  2.76trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000716 seconds\n",
      "[INFO][tpe.py:900] TPE using 38/38 trials with best loss -0.960000\n",
      " 39%|███▉      | 39/100 [00:22<00:21,  2.81trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.001628 seconds\n",
      "[INFO][tpe.py:900] TPE using 39/39 trials with best loss -0.960000\n",
      " 40%|████      | 40/100 [00:22<00:17,  3.35trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000945 seconds\n",
      "[INFO][tpe.py:900] TPE using 40/40 trials with best loss -0.960000\n",
      " 41%|████      | 41/100 [00:22<00:17,  3.45trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.001214 seconds\n",
      "[INFO][tpe.py:900] TPE using 41/41 trials with best loss -0.960000\n",
      " 42%|████▏     | 42/100 [00:23<00:21,  2.69trial/s, best loss: -0.96][INFO][tpe.py:864] build_posterior_wrapper took 0.000795 seconds\n",
      "[INFO][tpe.py:900] TPE using 42/42 trials with best loss -0.960000\n",
      " 43%|████▎     | 43/100 [00:23<00:17,  3.21trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.002033 seconds\n",
      "[INFO][tpe.py:900] TPE using 43/43 trials with best loss -0.960000\n",
      " 44%|████▍     | 44/100 [00:23<00:16,  3.45trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000795 seconds\n",
      "[INFO][tpe.py:900] TPE using 44/44 trials with best loss -0.960000\n",
      " 45%|████▌     | 45/100 [00:24<00:25,  2.17trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.002642 seconds\n",
      "[INFO][tpe.py:900] TPE using 45/45 trials with best loss -0.960000\n",
      " 46%|████▌     | 46/100 [00:24<00:22,  2.45trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000956 seconds\n",
      "[INFO][tpe.py:900] TPE using 46/46 trials with best loss -0.960000\n",
      " 47%|████▋     | 47/100 [00:25<00:20,  2.53trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001219 seconds\n",
      "[INFO][tpe.py:900] TPE using 47/47 trials with best loss -0.960000\n",
      " 48%|████▊     | 48/100 [00:25<00:16,  3.07trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000768 seconds\n",
      "[INFO][tpe.py:900] TPE using 48/48 trials with best loss -0.960000\n",
      " 49%|████▉     | 49/100 [00:25<00:18,  2.76trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000777 seconds\n",
      "[INFO][tpe.py:900] TPE using 49/49 trials with best loss -0.960000\n",
      " 50%|█████     | 50/100 [00:26<00:24,  2.07trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000760 seconds\n",
      "[INFO][tpe.py:900] TPE using 50/50 trials with best loss -0.960000\n",
      " 51%|█████     | 51/100 [00:26<00:18,  2.68trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000848 seconds\n",
      "[INFO][tpe.py:900] TPE using 51/51 trials with best loss -0.960000\n",
      " 52%|█████▏    | 52/100 [00:27<00:20,  2.33trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001870 seconds\n",
      "[INFO][tpe.py:900] TPE using 52/52 trials with best loss -0.960000\n",
      " 53%|█████▎    | 53/100 [00:27<00:16,  2.85trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000973 seconds\n",
      "[INFO][tpe.py:900] TPE using 53/53 trials with best loss -0.960000\n",
      " 54%|█████▍    | 54/100 [00:27<00:18,  2.50trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000935 seconds\n",
      "[INFO][tpe.py:900] TPE using 54/54 trials with best loss -0.960000\n",
      " 55%|█████▌    | 55/100 [00:28<00:15,  2.85trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000839 seconds\n",
      "[INFO][tpe.py:900] TPE using 55/55 trials with best loss -0.960000\n",
      " 56%|█████▌    | 56/100 [00:28<00:20,  2.10trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000802 seconds\n",
      "[INFO][tpe.py:900] TPE using 56/56 trials with best loss -0.960000\n",
      " 57%|█████▋    | 57/100 [00:29<00:25,  1.69trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000801 seconds\n",
      "[INFO][tpe.py:900] TPE using 57/57 trials with best loss -0.960000\n",
      " 58%|█████▊    | 58/100 [00:30<00:21,  1.98trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000835 seconds\n",
      "[INFO][tpe.py:900] TPE using 58/58 trials with best loss -0.960000\n",
      " 59%|█████▉    | 59/100 [00:30<00:19,  2.08trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000947 seconds\n",
      "[INFO][tpe.py:900] TPE using 59/59 trials with best loss -0.960000\n",
      " 60%|██████    | 60/100 [00:31<00:20,  1.92trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001241 seconds\n",
      "[INFO][tpe.py:900] TPE using 60/60 trials with best loss -0.960000\n",
      " 61%|██████    | 61/100 [00:31<00:20,  1.87trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000856 seconds\n",
      "[INFO][tpe.py:900] TPE using 61/61 trials with best loss -0.960000\n",
      " 62%|██████▏   | 62/100 [00:32<00:19,  1.92trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000834 seconds\n",
      "[INFO][tpe.py:900] TPE using 62/62 trials with best loss -0.960000\n",
      " 63%|██████▎   | 63/100 [00:32<00:17,  2.06trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.002145 seconds\n",
      "[INFO][tpe.py:900] TPE using 63/63 trials with best loss -0.960000\n",
      " 64%|██████▍   | 64/100 [00:32<00:16,  2.23trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000784 seconds\n",
      "[INFO][tpe.py:900] TPE using 64/64 trials with best loss -0.960000\n",
      " 65%|██████▌   | 65/100 [00:33<00:16,  2.15trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000744 seconds\n",
      "[INFO][tpe.py:900] TPE using 65/65 trials with best loss -0.960000\n",
      " 66%|██████▌   | 66/100 [00:33<00:13,  2.47trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000834 seconds\n",
      "[INFO][tpe.py:900] TPE using 66/66 trials with best loss -0.960000\n",
      " 67%|██████▋   | 67/100 [00:33<00:10,  3.06trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001084 seconds\n",
      "[INFO][tpe.py:900] TPE using 67/67 trials with best loss -0.960000\n",
      " 68%|██████▊   | 68/100 [00:34<00:11,  2.74trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000780 seconds\n",
      "[INFO][tpe.py:900] TPE using 68/68 trials with best loss -0.960000\n",
      " 69%|██████▉   | 69/100 [00:35<00:16,  1.84trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001005 seconds\n",
      "[INFO][tpe.py:900] TPE using 69/69 trials with best loss -0.960000\n",
      " 70%|███████   | 70/100 [00:35<00:14,  2.07trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000782 seconds\n",
      "[INFO][tpe.py:900] TPE using 70/70 trials with best loss -0.960000\n",
      " 71%|███████   | 71/100 [00:35<00:11,  2.51trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000793 seconds\n",
      "[INFO][tpe.py:900] TPE using 71/71 trials with best loss -0.960000\n",
      " 72%|███████▏  | 72/100 [00:36<00:10,  2.55trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000844 seconds\n",
      "[INFO][tpe.py:900] TPE using 72/72 trials with best loss -0.960000\n",
      "[INFO][tpe.py:864] build_posterior_wrapper took 0.001042 seconds\n",
      "[INFO][tpe.py:900] TPE using 73/73 trials with best loss -0.960000\n",
      " 74%|███████▍  | 74/100 [00:36<00:07,  3.57trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000837 seconds\n",
      "[INFO][tpe.py:900] TPE using 74/74 trials with best loss -0.960000\n",
      " 75%|███████▌  | 75/100 [00:36<00:06,  3.68trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001081 seconds\n",
      "[INFO][tpe.py:900] TPE using 75/75 trials with best loss -0.960000\n",
      " 76%|███████▌  | 76/100 [00:36<00:05,  4.03trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001140 seconds\n",
      "[INFO][tpe.py:900] TPE using 76/76 trials with best loss -0.960000\n",
      " 77%|███████▋  | 77/100 [00:37<00:05,  3.84trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000973 seconds\n",
      "[INFO][tpe.py:900] TPE using 77/77 trials with best loss -0.960000\n",
      " 78%|███████▊  | 78/100 [00:37<00:05,  3.98trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000898 seconds\n",
      "[INFO][tpe.py:900] TPE using 78/78 trials with best loss -0.960000\n",
      " 79%|███████▉  | 79/100 [00:37<00:06,  3.40trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000963 seconds\n",
      "[INFO][tpe.py:900] TPE using 79/79 trials with best loss -0.960000\n",
      " 80%|████████  | 80/100 [00:38<00:09,  2.04trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.011908 seconds\n",
      "[INFO][tpe.py:900] TPE using 80/80 trials with best loss -0.960000\n",
      " 81%|████████  | 81/100 [00:39<00:08,  2.37trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000784 seconds\n",
      "[INFO][tpe.py:900] TPE using 81/81 trials with best loss -0.960000\n",
      " 82%|████████▏ | 82/100 [00:39<00:07,  2.43trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001067 seconds\n",
      "[INFO][tpe.py:900] TPE using 82/82 trials with best loss -0.960000\n",
      " 83%|████████▎ | 83/100 [00:39<00:05,  2.87trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000777 seconds\n",
      "[INFO][tpe.py:900] TPE using 83/83 trials with best loss -0.960000\n",
      " 84%|████████▍ | 84/100 [00:39<00:05,  2.88trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000734 seconds\n",
      "[INFO][tpe.py:900] TPE using 84/84 trials with best loss -0.960000\n",
      " 85%|████████▌ | 85/100 [00:40<00:06,  2.50trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.005362 seconds\n",
      "[INFO][tpe.py:900] TPE using 85/85 trials with best loss -0.960000\n",
      " 86%|████████▌ | 86/100 [00:40<00:05,  2.41trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000814 seconds\n",
      "[INFO][tpe.py:900] TPE using 86/86 trials with best loss -0.960000\n",
      " 87%|████████▋ | 87/100 [00:41<00:04,  3.05trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000879 seconds\n",
      "[INFO][tpe.py:900] TPE using 87/87 trials with best loss -0.960000\n",
      " 88%|████████▊ | 88/100 [00:41<00:05,  2.29trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001230 seconds\n",
      "[INFO][tpe.py:900] TPE using 88/88 trials with best loss -0.960000\n",
      " 89%|████████▉ | 89/100 [00:42<00:04,  2.32trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000895 seconds\n",
      "[INFO][tpe.py:900] TPE using 89/89 trials with best loss -0.960000\n",
      " 90%|█████████ | 90/100 [00:42<00:04,  2.24trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.002198 seconds\n",
      "[INFO][tpe.py:900] TPE using 90/90 trials with best loss -0.960000\n",
      " 91%|█████████ | 91/100 [00:43<00:04,  1.92trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000972 seconds\n",
      "[INFO][tpe.py:900] TPE using 91/91 trials with best loss -0.960000\n",
      " 92%|█████████▏| 92/100 [00:43<00:03,  2.30trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000917 seconds\n",
      "[INFO][tpe.py:900] TPE using 92/92 trials with best loss -0.960000\n",
      " 93%|█████████▎| 93/100 [00:43<00:03,  2.30trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001148 seconds\n",
      "[INFO][tpe.py:900] TPE using 93/93 trials with best loss -0.960000\n",
      " 94%|█████████▍| 94/100 [00:44<00:02,  2.74trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000752 seconds\n",
      "[INFO][tpe.py:900] TPE using 94/94 trials with best loss -0.960000\n",
      " 95%|█████████▌| 95/100 [00:44<00:01,  3.17trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000993 seconds\n",
      "[INFO][tpe.py:900] TPE using 95/95 trials with best loss -0.960000\n",
      " 96%|█████████▌| 96/100 [00:44<00:01,  3.17trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000753 seconds\n",
      "[INFO][tpe.py:900] TPE using 96/96 trials with best loss -0.960000\n",
      " 97%|█████████▋| 97/100 [00:44<00:00,  3.44trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000756 seconds\n",
      "[INFO][tpe.py:900] TPE using 97/97 trials with best loss -0.960000\n",
      " 98%|█████████▊| 98/100 [00:45<00:00,  3.00trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.000728 seconds\n",
      "[INFO][tpe.py:900] TPE using 98/98 trials with best loss -0.960000\n",
      " 99%|█████████▉| 99/100 [00:45<00:00,  3.00trial/s, best loss: -0.9600000000000002][INFO][tpe.py:864] build_posterior_wrapper took 0.001061 seconds\n",
      "[INFO][tpe.py:900] TPE using 99/99 trials with best loss -0.960000\n",
      "100%|██████████| 100/100 [00:46<00:00,  2.17trial/s, best loss: -0.9600000000000002]\n",
      "Best hyperparameters found:\n",
      "n_estimators: 20\n",
      "max_depth: 5\n",
      "min_samples_split: 0.1535265522624955\n",
      "min_samples_leaf: 0.2079012119617774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params, X, y, cv):\n",
    "    params = {\n",
    "        'n_estimators': int(params['n_estimators']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'min_samples_split': params['min_samples_split'],\n",
    "        'min_samples_leaf': params['min_samples_leaf'],\n",
    "    }\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    acc = cross_val_score(clf, X, y, cv=cv).mean()\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 150, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 20, 1),\n",
    "    'min_samples_split': hp.uniform('min_samples_split', 0.1, 1),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5),\n",
    "} # If you have strong prior beliefs, sample around those beliefs with hp.normal instead\n",
    "''' Example of a search space with strong prior beliefs\n",
    "space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'naive_bayes',\n",
    "    },\n",
    "    {\n",
    "        'type': 'svm',\n",
    "        'C': hp.lognormal('svm_C', 0, 1),\n",
    "        'kernel': hp.choice('svm_kernel', [\n",
    "            {'ktype': 'linear'},\n",
    "            {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},\n",
    "            ]),\n",
    "    },\n",
    "    {\n",
    "        'type': 'dtree',\n",
    "        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\n",
    "        'max_depth': hp.choice('dtree_max_depth',\n",
    "            [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),\n",
    "        'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),\n",
    "    },\n",
    "])\n",
    "'''\n",
    "# Configure the optimization\n",
    "trials = Trials()\n",
    "algo = tpe.suggest  # Tree-structured Parzen Estimator (TPE) algorithm\n",
    "evals_per_optimization = 100\n",
    "\n",
    "# Run the optimization\n",
    "best = fmin(fn=partial(objective, X=X, y=y, cv=StratifiedKFold(5)),\n",
    "            space=space,\n",
    "            algo=algo,\n",
    "            max_evals=evals_per_optimization,\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng(1),\n",
    "            verbose=2)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(\"n_estimators:\", int(best[\"n_estimators\"]))\n",
    "print(\"max_depth:\", int(best[\"max_depth\"]))\n",
    "print(\"min_samples_split:\", best[\"min_samples_split\"])\n",
    "print(\"min_samples_leaf:\", best[\"min_samples_leaf\"])\n",
    "\n",
    "model_outputs['Hyperopt'] = {\n",
    "    \"n_estimators\": int(best[\"n_estimators\"]),\n",
    "    \"max_depth\": int(best[\"max_depth\"]),\n",
    "    \"min_samples_split\": best[\"min_samples_split\"],\n",
    "    \"min_samples_leaf\": best[\"min_samples_leaf\"],\n",
    "    \"loss\": trials.best_trial['result']['loss']\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Random Forest regressions (Github: [SMAC3](https://github.com/automl/SMAC3))\n",
    "\n",
    "`SMAC3` (Sequential Model-based Algorithm Configuration) is a versatile Bayesian optimization library that can use Random Forest regressions as surrogate models. `SMAC3` is particularly useful when dealing with high-dimensional, noisy, or expensive-to-evaluate objective functions. The library also provides features such as parallelization and support for categorical variables, making it a powerful tool for hyperparameter optimization.\n",
    "\n",
    "We will use the `SMAC3` library to optimize the hyperparameters of the Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:69] Using `n_configs` and ignoring `n_configs_per_hyperparameter`.\n",
      "[INFO][abstract_initial_design.py:134] Using 5 initial design configurations and 0 additional configurations.\n",
      "[INFO][abstract_intensifier.py:513] Added config 3ed8ce as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:588] Added config 4488bc and rejected config 3ed8ce as incumbent because it is not better than the incumbents on 3 instances:\n",
      "[INFO][configspace.py:175] --- max_depth: 13 -> 4\n",
      "[INFO][configspace.py:175] --- min_samples_leaf: 0.14300315268337727 -> 0.19232645876765278\n",
      "[INFO][configspace.py:175] --- min_samples_split: 0.29059861330315473 -> 0.2872218170976268\n",
      "[INFO][configspace.py:175] --- n_estimators: 131 -> 139\n",
      "[INFO][abstract_intensifier.py:588] Added config 192079 and rejected config 4488bc as incumbent because it is not better than the incumbents on 3 instances:\n",
      "[INFO][configspace.py:175] --- max_depth: 4 -> 19\n",
      "[INFO][configspace.py:175] --- min_samples_leaf: 0.19232645876765278 -> 0.1090451048433529\n",
      "[INFO][configspace.py:175] --- min_samples_split: 0.2872218170976268 -> 0.2572863092938888\n",
      "[INFO][configspace.py:175] --- n_estimators: 139 -> 60\n",
      "[INFO][abstract_intensifier.py:588] Added config 3bc4eb and rejected config 192079 as incumbent because it is not better than the incumbents on 3 instances:\n",
      "[INFO][configspace.py:175] --- max_depth: 19 -> 16\n",
      "[INFO][configspace.py:175] --- min_samples_leaf: 0.1090451048433529 -> 0.10482898496320926\n",
      "[INFO][configspace.py:175] --- min_samples_split: 0.2572863092938888 -> 0.12616483747725027\n",
      "[INFO][configspace.py:175] --- n_estimators: 60 -> 57\n",
      "[INFO][smbo.py:298] Finished 50 trials.\n",
      "[INFO][smbo.py:306] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:307] --- Remaining wallclock time: inf\n",
      "[INFO][smbo.py:308] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:309] --- Remaining trials: 0\n",
      "Best hyperparameters found:\n",
      "n_estimators: 57\n",
      "max_depth: 16\n",
      "min_samples_split: 0.12616483747725027\n",
      "min_samples_leaf: 0.10482898496320926\n"
     ]
    }
   ],
   "source": [
    "class RandomForest:\n",
    "    # Define the objective function\n",
    "    @property\n",
    "    def configspace(self) -> ConfigurationSpace:\n",
    "        \"\"\"Note you can create dependencies between hyperparameters. Quite cool!\n",
    "            class is `InCondition`, and you pass these onto the ConfigurationSpace via \n",
    "            `cs.add_conditions`\n",
    "\n",
    "        Returns:\n",
    "            ConfigurationSpace: A configuration space object.\n",
    "        \"\"\"        \n",
    "        cs = ConfigurationSpace()\n",
    "        n_estimators = UniformIntegerHyperparameter('n_estimators', 10, 150)\n",
    "        max_depth = UniformIntegerHyperparameter('max_depth', 1, 20)\n",
    "        min_samples_split = UniformFloatHyperparameter('min_samples_split', 0.1, 1)\n",
    "        min_samples_leaf = UniformFloatHyperparameter('min_samples_leaf', 0.1, 0.5)\n",
    "        criterion = CategoricalHyperparameter('criterion', ['gini', 'entropy'])\n",
    "        cs.add_hyperparameters([n_estimators, max_depth, min_samples_split, min_samples_leaf, criterion])\n",
    "        return cs\n",
    "    \n",
    "    def train(self, config: Configuration, seed: int = 0) -> float:\n",
    "        config_dict = config.get_dictionary()\n",
    "        clf = RandomForestClassifier(**config_dict, random_state=seed)\n",
    "        acc = cross_val_score(clf, X, y).mean()\n",
    "        return -acc\n",
    "    \n",
    "classifier = RandomForest()\n",
    "\n",
    "# we create an object, holding general information about the run\n",
    "scenario = Scenario(\n",
    "    classifier.configspace,\n",
    "    n_trials=50,  # We want to run max 50 trials (combination of config and seed)\n",
    ")\n",
    "\n",
    "# We want to run the facade's default initial design\n",
    "initial_design = HyperparameterOptimizationFacade.get_initial_design(scenario, n_configs=5)\n",
    "\n",
    "# Now we use SMAC to find the best hyperparameters\n",
    "smac = HyperparameterOptimizationFacade(\n",
    "    scenario,\n",
    "    classifier.train,\n",
    "    initial_design=initial_design,\n",
    "    overwrite=True,  # If the run exists, we overwrite it; alternatively, we can continue from last state\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "incumbent = smac.optimize()\n",
    "\n",
    "# Compute loss outside incumbent\n",
    "inc_value = classifier.train(incumbent)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(\"n_estimators:\", int(incumbent[\"n_estimators\"]))\n",
    "print(\"max_depth:\", int(incumbent[\"max_depth\"]))\n",
    "print(\"min_samples_split:\", incumbent[\"min_samples_split\"])\n",
    "print(\"min_samples_leaf:\", incumbent[\"min_samples_leaf\"])\n",
    "\n",
    "model_outputs['SMAC'] = {\n",
    "    \"n_estimators\": int(incumbent[\"n_estimators\"]),\n",
    "    \"max_depth\": int(incumbent[\"max_depth\"]),\n",
    "    \"min_samples_split\": incumbent[\"min_samples_split\"],\n",
    "    \"min_samples_leaf\": incumbent[\"min_samples_leaf\"],\n",
    "    \"loss\": - classifier.train(incumbent)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running each of the four models, let's present a table comparing their relative performance in terms of the optimized hyperparameters and cross-validated accuracy. This comparison provides insights into the effectiveness of each method for hyperparameter optimization, and help us make an informed decision about which method to use in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomizedSearchCV</th>\n",
       "      <th>BayesianOptimization</th>\n",
       "      <th>Hyperopt</th>\n",
       "      <th>SMAC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>57.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_split</th>\n",
       "      <td>0.163784</td>\n",
       "      <td>0.502717</td>\n",
       "      <td>0.153527</td>\n",
       "      <td>0.126165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <td>0.198007</td>\n",
       "      <td>0.147383</td>\n",
       "      <td>0.207901</td>\n",
       "      <td>0.104829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>-0.946667</td>\n",
       "      <td>-0.960000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMAC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   RandomizedSearchCV  BayesianOptimization   Hyperopt   \n",
       "n_estimators                49.000000            149.000000  20.000000  \\\n",
       "max_depth                    2.000000              1.000000   5.000000   \n",
       "min_samples_split            0.163784              0.502717   0.153527   \n",
       "min_samples_leaf             0.198007              0.147383   0.207901   \n",
       "loss                        -0.946667             -0.960000   0.960000   \n",
       "SMAC                              NaN                   NaN        NaN   \n",
       "\n",
       "                        SMAC  \n",
       "n_estimators       57.000000  \n",
       "max_depth          16.000000  \n",
       "min_samples_split   0.126165  \n",
       "min_samples_leaf    0.104829  \n",
       "loss                     NaN  \n",
       "SMAC                     NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table with model outputs\n",
    "model_outputs = pd.DataFrame(model_outputs).T\n",
    "model_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to bear in mind:\n",
    "- Model overfitting: While Bayesian Optimization is less prone to overfitting compared to exhaustive search methods like grid search, it's still possible to overfit the model to the training data when searching for optimal hyperparameters. To prevent this, always use cross-validation during the optimization process. This provides a more reliable estimate of the model's performance on unseen data, which can help reduce the risk of overfitting.\n",
    "\n",
    "- Complex surrogate model: Bayesian Optimization relies on a surrogate model (e.g., Gaussian Processes) to approximate the objective function. If this model is too complex, it may overfit the observed data, leading to overly optimistic predictions in certain regions of the search space. This could cause the optimization process to focus on regions that are not truly optimal. To reduce the risk of overfitting the surrogate model, it's important to choose an appropriate kernel function and its hyperparameters. In practice, a simpler kernel (e.g., a Radial Basis Function with fewer hyperparameters) is often preferred to avoid overfitting.\n",
    "\n",
    "- Noise in the objective function: If the objective function is noisy, the surrogate model may struggle to fit the underlying trend, which could lead to suboptimal optimization. To address this, you can use techniques such as averaging multiple evaluations at the same point or using a more robust kernel that can handle noise better (e.g., the Matérn kernel).\n",
    "\n",
    "- Number of iterations and exploration-exploitation trade-off: It is important to balance the number of iterations and the exploration-exploitation trade-off. Running too few iterations may lead to suboptimal results, while running too many iterations may result in over-exploitation of certain regions in the search space. Adjusting the exploration-exploitation parameters of the acquisition function (e.g., kappa for UCB and xi for EI and POI) can help balance this trade-off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rvo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
